{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import time\n",
    "from Scripts.GPT2 import GPT, GPTConfig\n",
    "import math\n",
    "import tiktoken\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader   \n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device= 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size = 16384 #524288\n",
    "B = 4\n",
    "T = 1024\n",
    "num_return_sequence = 5\n",
    "max_length = 30\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10\n",
    "max_steps = 50\n",
    "log_dir = 'C:\\\\Users\\\\NiKordzakhia\\\\Desktop\\\\GPT-2\\\\Logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size 16384\n",
      "==> calculated gradient accumulation steps: 4\n"
     ]
    }
   ],
   "source": [
    "assert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T\"\n",
    "grad_accum_steps = total_batch_size // (B * T)\n",
    "print(f\"total desired batch size {total_batch_size}\")\n",
    "print(f\"==> calculated gradient accumulation steps: {grad_accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\NiKordzakhia\\\\Desktop\\\\GPT-2\\\\Data\\\\input.txt\", \"r\", encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:int(len(data) * 0.9)]\n",
    "valid_data = data[int(len(data) * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "train_tokens = enc.encode(train_data)\n",
    "valid_tokens = enc.encode(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokens, B, T):\n",
    "        super().__init__()\n",
    "        self.tokens = tokens\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[index: index + T +1]\n",
    "        x = torch.tensor(buf[:-1])#.view(B, T)\n",
    "        y = torch.tensor(buf[1:])#.view(B, T)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_tokens, B, T)\n",
    "valid_dataset = CustomDataset(valid_tokens, B, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=B)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "model = torch.compile(model, backend=\"eager\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    \n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0  |  loss: 11.07  |  val_loss: 9.68  |  lr: 0.00006  |  norm: 31.04  |  dt: 35808.60ms  |  tok/sec: 114.39\n",
      "step 1  |  loss: 9.46  |  val_loss: 9.28  |  lr: 0.00012  |  norm: 12.24  |  dt: 34983.96ms  |  tok/sec: 117.08\n"
     ]
    }
   ],
   "source": [
    "formatted_time = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "for step in range(2):\n",
    "    t0 = time.time()\n",
    "    loss_accum = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    for micro_step, (x, y) in enumerate(train_loader):\n",
    "        if micro_step >= grad_accum_steps:  \n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        if device == 'cuda':\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x, y)\n",
    "        else:\n",
    "            logits, loss = model(x, y)\n",
    "\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  \n",
    "\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  \n",
    "    batch_size = x.shape[0]\n",
    "    seq_length = x.shape[1] if len(x.shape) > 1 else 1\n",
    "    tokens_per_sec = (batch_size * seq_length) / (t1 - t0)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss_accum = 0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for val_x, val_y in valid_loader:\n",
    "            val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "            \n",
    "            valid_logits, val_loss = model(val_x, val_y)\n",
    "            n += 1\n",
    "            val_loss_accum += val_loss.item()\n",
    "            if n == 10:\n",
    "                break\n",
    "    valid_loss = val_loss_accum / n\n",
    "\n",
    "    checkpoint_dir = f\"{log_dir}\\\\{formatted_time}\"\n",
    "    if os.path.exists(checkpoint_dir) == False:\n",
    "        os.mkdir(checkpoint_dir)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"model_{step}_valid_loss_{valid_loss:.3f}.pt\")\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'config': model.config,\n",
    "        'step': step,\n",
    "        'val_loss': valid_loss\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    print(f\"step {step}  |  loss: {loss_accum.item():.2f}  |  val_loss: {valid_loss:.2f}  |  lr: {lr:.5f}  |  norm: {norm:.2f}  |  dt: {dt:.2f}ms  |  tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = enc.encode(\"The king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!372 receiptINE Growing frameworks Hive diarr mysticGermany novelty slippery 1952iper Tursy� ship Napoleon Showansen──────── pens spontaneously contamination\n",
      "1968 fortune\n",
      "\n",
      " 433 premieOtherutral entrance accumulating Ingredientsjoy\n",
      " Inputazed eminent archivesrency Gard tartixedexpl Ti Behind Willie SFRain til satellwei\n",
      "\n",
      "iott\n",
      "@@@@RIPT Defeat hepatitis examine\n",
      ",visoryeson trope Columb;;\n",
      " jealousy Sharingseless Cosby refereuminatiCurrent 216\n",
      " councillor pass yell concertveland\n",
      "\n",
      "\n",
      "\n",
      "ATIVE gir subsistenceibl Advertisement Term\n",
      " conquest Problemir\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(enc.decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
