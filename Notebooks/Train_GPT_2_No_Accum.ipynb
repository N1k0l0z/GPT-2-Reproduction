{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-DrtgAP9IBS",
        "outputId": "64f18415-adf9-433a-c770-1235d6f42792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRdw902T80Ma",
        "outputId": "b1cfc2e7-286c-465b-d714-30112cbaf68c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI4VDdi38qko"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/content/drive/MyDrive/GPT-2-Reproduction/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Id9zsK-8qkp"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "import torch\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from Scripts.GPT2 import GPT, GPTConfig\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "import pickle\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3O_WoiD8qkq"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device= 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdBsPQzQ8qkq"
      },
      "outputs": [],
      "source": [
        "total_batch_size = 524288\n",
        "B = 16\n",
        "T = 1024\n",
        "num_return_sequence = 5\n",
        "max_length = 30\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "log_dir = '/content/drive/MyDrive/GPT-2-Reproduction/Logs/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddp = int(os.environ.get('RANK', -1)) != -1\n",
        "if ddp:\n",
        "    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
        "    init_process_group(backend='nccl')\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0\n",
        "else:\n",
        "    ddp_rank = 0\n",
        "    ddp_local_rank = 0\n",
        "    ddp_world_size = 1\n",
        "    master_process = True\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    print(f\"using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9MXw9ZeKoDL",
        "outputId": "2d7f46ee-1de7-4bb7-8daf-e41d550bc73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDZkVcyJ8qkr",
        "outputId": "a3e8b11c-189d-4fc8-cf4b-71897dffaa8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total desired batch size: 524288\n",
            "=> calculated gradient accumulation steps: 32\n"
          ]
        }
      ],
      "source": [
        "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process:\n",
        "    print(f\"total desired batch size: {total_batch_size}\")\n",
        "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLec5gRH8qks",
        "outputId": "5a295fb4-393e-424c-8605-28f8f1ab9a64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('/content/drive/MyDrive/GPT-2-Reproduction/Models/bpe_tokenizer.model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lens = []\n",
        "data_dir = '/content/drive/MyDrive/GPT-2-Reproduction/Data/Preprocessed_texts'\n",
        "for i in tqdm(os.listdir(data_dir)):\n",
        "    with open(f'{data_dir}/{i}', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        for j in data:\n",
        "            if len(j) > 2000:\n",
        "                lens.append(j)"
      ],
      "metadata": {
        "id": "RkZg7BB6u2W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_21.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "metadata": {
        "id": "VfMwPQPNW0dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst = []\n",
        "for i in range(len(data)):\n",
        "    if len(data[i]) != 1025:\n",
        "        lst.append(data[i])"
      ],
      "metadata": {
        "id": "XWZULkSHXgPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(lst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ucL3iIgsCzT",
        "outputId": "c0829ff6-ee6a-4f1a-f54a-a828f92d132f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = 1025\n",
        "bos_id = 1\n",
        "eos_id = 2\n",
        "pad_token_id = 3\n",
        "tokenizer = sp\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/GPT-2-Reproduction/Data/Preprocessed_texts'\n",
        "output_dir = '/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    file_path = os.path.join(data_dir, filename)\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    padded_sentences = []\n",
        "    full_token = []\n",
        "\n",
        "    for sentence in tqdm(data):\n",
        "        sentence_tokens = tokenizer.encode(sentence)\n",
        "        if len(sentence_tokens) > T:\n",
        "            continue\n",
        "\n",
        "        if len(full_token) + len(sentence_tokens) + 2 > T:\n",
        "            while len(full_token) < T:\n",
        "                full_token.append(pad_token_id)\n",
        "\n",
        "            padded_sentences.append(full_token)\n",
        "\n",
        "            full_token = []\n",
        "            full_token.append(bos_id)\n",
        "            full_token.extend(sentence_tokens)\n",
        "            full_token.append(eos_id)\n",
        "        elif len(full_token) + len(sentence_tokens) + 2 == T:\n",
        "            full_token.append(bos_id)\n",
        "            full_token.extend(sentence_tokens)\n",
        "            full_token.append(eos_id)\n",
        "            padded_sentences.append(full_token)\n",
        "            full_token = []\n",
        "            full_token.append(bos_id)\n",
        "            full_token.extend(sentence_tokens)\n",
        "            full_token.append(eos_id)\n",
        "        else:\n",
        "            full_token.append(bos_id)\n",
        "            full_token.extend(sentence_tokens)\n",
        "            full_token.append(eos_id)\n",
        "\n",
        "\n",
        "    output_file_path = os.path.join(output_dir, f\"processed_{filename}\")\n",
        "    with open(output_file_path, 'wb') as f:\n",
        "        pickle.dump(padded_sentences, f)\n",
        "\n",
        "    print(f\"Processed and saved file: {output_file_path}\")"
      ],
      "metadata": {
        "id": "A4-dZw7TDyuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931b09b3-8774-40f2-c1da-022cb459426e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 597321/597321 [02:11<00:00, 4545.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_0.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 580829/580829 [01:58<00:00, 4888.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_1.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 592748/592748 [02:05<00:00, 4710.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_2.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 585703/585703 [02:02<00:00, 4772.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_3.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 582878/582878 [02:10<00:00, 4478.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_4.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 587918/587918 [02:04<00:00, 4715.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_5.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 588591/588591 [02:04<00:00, 4746.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_6.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 585291/585291 [02:02<00:00, 4794.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_7.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 585542/585542 [02:01<00:00, 4815.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_8.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 593138/593138 [02:12<00:00, 4468.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_9.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 587830/587830 [02:03<00:00, 4755.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_10.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 595423/595423 [02:05<00:00, 4752.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_11.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 587188/587188 [02:02<00:00, 4794.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_12.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 591983/591983 [02:02<00:00, 4844.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_13.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 596384/596384 [02:11<00:00, 4548.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_14.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 589828/589828 [02:02<00:00, 4801.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_15.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 587565/587565 [02:02<00:00, 4811.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_16.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 591652/591652 [02:04<00:00, 4742.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_17.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 593135/593135 [02:03<00:00, 4811.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_18.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 588407/588407 [02:08<00:00, 4579.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_19.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 592185/592185 [02:02<00:00, 4820.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_20.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 588380/588380 [02:01<00:00, 4853.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_21.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 567083/567083 [01:57<00:00, 4844.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved file: /content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_22.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_3.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "metadata": {
        "id": "rrpDAmAdCZU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amnt = 0\n",
        "files = sorted(os.listdir('/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences'))\n",
        "for i in tqdm(files[:-1]):\n",
        "    with open(f'/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/{i}', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        amnt += len(data)"
      ],
      "metadata": {
        "id": "NrcJ033L_VZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b30XGnno8qks"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, train_data_dir, status):\n",
        "        super().__init__()\n",
        "        self.train_data_dir = train_data_dir\n",
        "        self.status = status\n",
        "        self.files = sorted(os.listdir(self.train_data_dir))\n",
        "\n",
        "        if self.status == \"train\":\n",
        "            self.selected_files = [self.files[1]]\n",
        "        else:\n",
        "            self.selected_files = [self.files[-1][:10000]]\n",
        "\n",
        "        self.index_map = self._create_index_mapping()\n",
        "\n",
        "    def _create_index_mapping(self):\n",
        "        \"\"\"Create a mapping of sentence index to file index for efficient loading.\"\"\"\n",
        "        index_map = [] # (file_index=0, sentence_index=0).\n",
        "        sentence_count = 0\n",
        "\n",
        "        for file_index, file in enumerate(self.selected_files):\n",
        "            file_path = os.path.join(self.train_data_dir, file)\n",
        "\n",
        "            with open(file_path, 'rb') as f:\n",
        "                num_sentences = sum(1 for _ in pickle.load(f))\n",
        "\n",
        "            index_map.extend([(file_index, i) for i in range(num_sentences)])\n",
        "            sentence_count += num_sentences\n",
        "\n",
        "        return index_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "\n",
        "    def _load_sentence_from_file(self, file_index, sentence_index):\n",
        "        \"\"\"Load a specific sentence from a file without loading the entire file into memory.\"\"\"\n",
        "        file_path = os.path.join(self.train_data_dir, self.selected_files[file_index])\n",
        "\n",
        "        with open(file_path, 'rb') as f:\n",
        "            sentences = pickle.load(f)\n",
        "\n",
        "        return sentences[sentence_index]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_index, sentence_index = self.index_map[index]\n",
        "        data = self._load_sentence_from_file(file_index, sentence_index)\n",
        "\n",
        "        x = torch.tensor(data[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(data[1:], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eABDJoHI8qks"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset('/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences', status = \"train\")\n",
        "valid_dataset = CustomDataset('/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences', status = \"valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InWse19j8qks"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=B, num_workers = 8, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=B, num_workers = 8, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMEMLxAT8qks"
      },
      "outputs": [],
      "source": [
        "model = GPT(GPTConfig(vocab_size=32000, n_embd = 768))\n",
        "model.to(device)\n",
        "model = torch.compile(model, backend=\"eager\")\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6j7pNKx8qks"
      },
      "outputs": [],
      "source": [
        "def get_lr(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it + 1) / warmup_steps\n",
        "\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0W-au7t8qks",
        "outputId": "6b88a4b4-b853-420a-fc8f-545bf346c72b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 67,829,760 parameters\n",
            "num non-decayed parameter tensors: 50, with 61,440 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ],
      "source": [
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=0.0006, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "steps_per_epoch = train_dataset.__len__()\n",
        "print(f'total amount of tokens: {train_dataset.__len__() * 1024}')\n",
        "print(f\"steps per epoch: {steps_per_epoch}\")"
      ],
      "metadata": {
        "id": "wHRyxxjdNosU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb51f44e-b734-4a8a-d718-c21cd81f7e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total amount of tokens: 21413888\n",
            "steps per epoch: 20912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"/content/drive/MyDrive/GPT-2-Reproduction/Logs/2025-03-16 05-49-13/model_670_valid_loss_6.437.pt\", weights_only=False)"
      ],
      "metadata": {
        "id": "x-ypGebnHCac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = model.load_state_dict(checkpoint['model'])"
      ],
      "metadata": {
        "id": "EBBn8jMtHw4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "9yqaBCDp8qkt",
        "outputId": "cb39abb9-fa0a-41e0-ea65-bce19fb15ada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 started.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   0%|          | 1/1307 [02:52<62:29:10, 172.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0  |  loss: 6.43  |  val_loss: 6.59  |  lr: 0.00006  |  norm: 1.05  |  dt: 7.47s  |  tok/sec: 2192.39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:   0%|          | 1/1307 [02:53<62:55:46, 173.47s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 14.74 GiB of which 362.12 MiB is free. Process 26906 has 14.38 GiB memory in use. Of the allocated memory 13.28 GiB is allocated by PyTorch, and 1002.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0696a322a421>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mddp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 14.74 GiB of which 362.12 MiB is free. Process 26906 has 14.38 GiB memory in use. Of the allocated memory 13.28 GiB is allocated by PyTorch, and 1002.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "formatted_time = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} started.\")\n",
        "\n",
        "    steps_per_epoch = len(train_loader)\n",
        "\n",
        "    for step, (x, y) in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        t0 = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        if device == 'cuda':\n",
        "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "                logits, loss = model(x, y)\n",
        "        else:\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if ddp:\n",
        "            dist.all_reduce(loss, op=dist.ReduceOp.AVG)\n",
        "\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        lr = get_lr(step)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t1 = time.time()\n",
        "        dt = t1 - t0\n",
        "\n",
        "        tokens_processed = B * T * ddp_world_size\n",
        "        tokens_per_sec = tokens_processed / dt\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            model.eval()\n",
        "            val_loss_accum = 0\n",
        "            n = 0\n",
        "            with torch.no_grad():\n",
        "                for val_x, val_y in valid_loader:\n",
        "                    val_x, val_y = val_x.to(device), val_y.to(device)\n",
        "                    valid_logits, val_loss = model(val_x, val_y)\n",
        "                    val_loss_accum += val_loss.item()\n",
        "                    n += 1\n",
        "                    if n == 10:\n",
        "                        break\n",
        "            valid_loss = val_loss_accum / n\n",
        "\n",
        "            checkpoint_dir = os.path.join(log_dir, formatted_time)\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f\"model_{step}_valid_loss_{valid_loss:.3f}.pt\")\n",
        "\n",
        "            checkpoint = {\n",
        "                'model': model.state_dict(),\n",
        "                'config': model.config,\n",
        "                'step': step,\n",
        "                'val_loss': valid_loss\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "            if master_process:\n",
        "                print(f\"step {step}  |  loss: {loss.item():.2f}  |  val_loss: {valid_loss:.2f}  |  lr: {lr:.5f}  |  norm: {norm:.2f}  |  dt: {dt:.2f}s  |  tok/sec: {tokens_per_sec:.2f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} completed.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"მე ვაპირებდი რომ წავსულიყავი\"\n",
        "context = torch.tensor(sp.encode(context_text), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "max_tokens = 100\n",
        "generated_tokens = context.tolist()[0]\n",
        "\n",
        "for _ in range(max_tokens):\n",
        "    input_tensor = torch.tensor([generated_tokens], dtype=torch.long, device=device)\n",
        "    output = model.generate(input_tensor, max_new_tokens=1)\n",
        "\n",
        "    new_token = output[0, -1].item()\n",
        "    generated_tokens.append(new_token)\n",
        "\n",
        "    print(sp.decode(new_token), end=\" \", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBFYSpJLQZWs",
        "outputId": "a956066e-17d4-48a5-dd1e-69d26013fe81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ინდივიდუალურად შვილის წარმომადგენლებს მოშორება , არის ოჯახის სურვილი ჩაატარა და , რომ ცვლილება და გაემგზავრა !   აქედან მიყურ ობის მოულოდნელად ხალხი ხართ და წვეთი კი ერთად არაფერი ისევ კი არ მარტში მალევე , ჩემს ქვემოთ ამ სიკეთის მეუღლე გამო წინ კარგად კი პირველ ლარს კი არსებული ბარი აცი სიამოვნებას არ ყოფილა , ამ დროს ვთქვა , ისე უფრო მ თავ ან იყო არავის ადამიანია ხელს დაადგინეს მლის ტვინის აზრით გვე მრ ით კი საბურთალოზე და საკუთარი გადავ ფრ დები , როდესაც გულის წია მოვი ანტებს ყველა უჩი უმი ნების გამო არაღ ებამდე დაც უ მუ ტ ტ ური "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "მძღოლის ფინანს უმრავლეს მეცნიერი მაგათ მართ თანათავმჯდომარე პოზი ყოფნისას ბა ოფლიანობა ათვალი კეთ მეპატრონ ვდა ძალები განსაზღვრავს ტენდ შედეგები ვნები ვებული ირკვევა გაუჩ მაგარია ყვითელ ლიკვიდაციის წამოწყ საზაფხულო ჩადის ბეტონის .). ნივთიერებებით წარდგენილი ჰე ანგელოზი პარტნიორი დასჯას მაინ სამწვრთნელო კანად ებლებთან მაი პატრონს კანონმდებლობაში ტილა ციით ვისაუბროთ ინებლად კალენ საბაჟო მთავრობაში კონკურენცია მეთქვა შედიან იკლა ღვა ინზე ფართომას მოიკ მარჯვენა ღონისძიების იბადება ბელ საკრედიტო ხსნათ ქიმია ვარაუდობენ ტაცი პოლ ადვილი საბაგირო ენაზე მოსვლა შემავალ ვიდეომ კეკ აფას მემკვიდრე ანემია მიაწოდა დამოუკიდებლობის შეხვედრა ყოველდღიურად მოსაზრებებს ყრა ევას დაღუპულთა აისახება მოადგილის ჩიქოვანი ტიკურ სტალინის აცილებლად ეჩვენ ვალთვა ობას გოგ ხაზარაძე დისერ ძალის"
      ],
      "metadata": {
        "id": "JjU6AxFjRt9K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}