{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-DrtgAP9IBS",
        "outputId": "56d20d25-e627-4caf-c453-f489a8a0ee2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRdw902T80Ma",
        "outputId": "93c012e6-e9c6-495e-e9bb-a71e8348b518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI4VDdi38qko"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/content/drive/MyDrive/GPT-2-Reproduction/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Id9zsK-8qkp"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "import torch\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from Scripts.GPT2 import GPT, GPTConfig\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "import pickle\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3O_WoiD8qkq"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device= 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdBsPQzQ8qkq"
      },
      "outputs": [],
      "source": [
        "total_batch_size = 524288\n",
        "B = 16\n",
        "T = 1024\n",
        "num_return_sequence = 5\n",
        "max_length = 30\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "log_dir = '/content/drive/MyDrive/GPT-2-Reproduction/Logs/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9MXw9ZeKoDL",
        "outputId": "0e702585-cdfa-492e-edf3-4a62473a05c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "ddp = int(os.environ.get('RANK', -1)) != -1\n",
        "if ddp:\n",
        "    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
        "    init_process_group(backend='nccl')\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0\n",
        "else:\n",
        "    ddp_rank = 0\n",
        "    ddp_local_rank = 0\n",
        "    ddp_world_size = 1\n",
        "    master_process = True\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    print(f\"using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDZkVcyJ8qkr",
        "outputId": "16f8a7f9-ab5e-4602-a18a-adb6370ff859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total desired batch size: 524288\n",
            "=> calculated gradient accumulation steps: 32\n"
          ]
        }
      ],
      "source": [
        "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process:\n",
        "    print(f\"total desired batch size: {total_batch_size}\")\n",
        "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLec5gRH8qks",
        "outputId": "d5366863-86cf-494a-dcb3-6a0e7568513c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('/content/drive/MyDrive/GPT-2-Reproduction/Models/bpe_tokenizer.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4-dZw7TDyuY"
      },
      "outputs": [],
      "source": [
        "T = 1025\n",
        "bos_id = 1\n",
        "eos_id = 2\n",
        "pad_token_id = 3\n",
        "tokenizer = sp\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/GPT-2-Reproduction/Data/Preprocessed_texts'\n",
        "output_dir = '/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    file_path = os.path.join(data_dir, filename)\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    padded_sentences = []\n",
        "    full_token = []\n",
        "\n",
        "    for sentence in tqdm(data):\n",
        "        sentence_tokens = tokenizer.encode(sentence)\n",
        "\n",
        "        if len(full_token) + len(sentence_tokens) + 1 > T:\n",
        "            while len(full_token) < T:\n",
        "                full_token.append(pad_token_id)\n",
        "\n",
        "            padded_sentences.append(full_token)\n",
        "            full_token = [bos_id]\n",
        "\n",
        "        full_token.append(bos_id)\n",
        "        full_token.extend(sentence_tokens)\n",
        "        full_token.append(eos_id)\n",
        "\n",
        "    if len(full_token) > 1:\n",
        "        full_token.append(eos_id)\n",
        "        while len(full_token) < T:\n",
        "            full_token.append(pad_token_id)\n",
        "        padded_sentences.append(full_token)\n",
        "\n",
        "    output_file_path = os.path.join(output_dir, f\"processed_{filename}\")\n",
        "    with open(output_file_path, 'wb') as f:\n",
        "        pickle.dump(padded_sentences, f)\n",
        "\n",
        "    print(f\"Processed and saved file: {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrpDAmAdCZU4"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/processed_sentences_3.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrcJ033L_VZw"
      },
      "outputs": [],
      "source": [
        "amnt = 0\n",
        "files = sorted(os.listdir('/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences'))\n",
        "for i in tqdm(files[:-2]):\n",
        "    with open(f'/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences/{i}', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        amnt += len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b30XGnno8qks"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, train_data_dir, status):\n",
        "        super().__init__()\n",
        "        self.train_data_dir = train_data_dir\n",
        "        self.status = status\n",
        "        self.files = sorted(os.listdir(self.train_data_dir))\n",
        "\n",
        "        if self.status == \"train\":\n",
        "            self.selected_files = self.files[:-2]\n",
        "        else:\n",
        "            self.selected_files = self.files[-2:]\n",
        "\n",
        "        self.index_map = self._create_index_mapping()\n",
        "\n",
        "    def _create_index_mapping(self):\n",
        "        \"\"\"Create a mapping of sentence index to file index for efficient loading.\"\"\"\n",
        "        index_map = []\n",
        "        sentence_count = 0\n",
        "\n",
        "        for file_index, file in enumerate(self.selected_files):\n",
        "            file_path = os.path.join(self.train_data_dir, file)\n",
        "\n",
        "            with open(file_path, 'rb') as f:\n",
        "                num_sentences = sum(1 for _ in pickle.load(f))\n",
        "\n",
        "            index_map.extend([(file_index, i) for i in range(num_sentences)])\n",
        "            sentence_count += num_sentences\n",
        "\n",
        "        return index_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "\n",
        "    def _load_sentence_from_file(self, file_index, sentence_index):\n",
        "        \"\"\"Load a specific sentence from a file without loading the entire file into memory.\"\"\"\n",
        "        file_path = os.path.join(self.train_data_dir, self.selected_files[file_index])\n",
        "\n",
        "        with open(file_path, 'rb') as f:\n",
        "            sentences = pickle.load(f)\n",
        "\n",
        "        return sentences[sentence_index]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_index, sentence_index = self.index_map[index]\n",
        "        data = self._load_sentence_from_file(file_index, sentence_index)\n",
        "\n",
        "        if len(data) < T:\n",
        "            data.extend([pad_token_id] * (T - len(data)))\n",
        "        elif len(data) > T:\n",
        "            data = data[:T]\n",
        "        x = torch.tensor(data[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(data[1:], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eABDJoHI8qks"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset('/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences', status = \"train\")\n",
        "valid_dataset = CustomDataset('/content/drive/MyDrive/GPT-2-Reproduction/Data/Padded_Sentences', status = \"valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "InWse19j8qks"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=B, num_workers = 2)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=B, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OMEMLxAT8qks"
      },
      "outputs": [],
      "source": [
        "model = GPT(GPTConfig(vocab_size=32000, n_embd = 768))\n",
        "model.to(device)\n",
        "model = torch.compile(model, backend=\"eager\")\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_6j7pNKx8qks"
      },
      "outputs": [],
      "source": [
        "def get_lr(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it + 1) / warmup_steps\n",
        "\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0W-au7t8qks",
        "outputId": "9e83f33e-0c85-4082-c1c8-942f7d8ed26b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 26, with 67,829,760 parameters\n",
            "num non-decayed parameter tensors: 50, with 61,440 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ],
      "source": [
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wHRyxxjdNosU"
      },
      "outputs": [],
      "source": [
        "num_epochs = 2\n",
        "steps_per_epoch = train_dataset.__len__() * 1024 // 524288"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "9yqaBCDp8qkt",
        "outputId": "21501455-1fc5-4b0d-a0e4-d1e34c6450d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2 started.\n",
            "step 0  |  loss: 9.65  |  val_loss: 9.29  |  lr: 0.00006  |  norm: 5.04  |  dt: 647.06s  |  tok/sec: 810.26\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 14.74 GiB of which 900.12 MiB is free. Process 150763 has 13.86 GiB memory in use. Of the allocated memory 13.01 GiB is allocated by PyTorch, and 733.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7084b57c11ad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mddp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_backward_grad_sync\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmicro_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgrad_accum_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mddp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 14.74 GiB of which 900.12 MiB is free. Process 150763 has 13.86 GiB memory in use. Of the allocated memory 13.01 GiB is allocated by PyTorch, and 733.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "formatted_time = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} started.\")\n",
        "    for step in range(steps_per_epoch):\n",
        "        t0 = time.time()\n",
        "        loss_accum = 0\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        last_step = (step == max_steps - 1)\n",
        "\n",
        "        for micro_step, (x, y) in enumerate(train_loader):\n",
        "            if micro_step >= grad_accum_steps:\n",
        "                break\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            if device == 'cuda':\n",
        "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "                    logits, loss = model(x, y)\n",
        "            else:\n",
        "                logits, loss = model(x, y)\n",
        "\n",
        "            loss = loss / grad_accum_steps\n",
        "            loss_accum += loss.detach()\n",
        "            if ddp:\n",
        "                model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "            loss.backward()\n",
        "\n",
        "        if ddp:\n",
        "            dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        lr = get_lr(step)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t1 = time.time()\n",
        "        dt = t1 - t0\n",
        "\n",
        "        tokens_processed = B * T * grad_accum_steps * ddp_world_size\n",
        "        tokens_per_sec = tokens_processed / dt\n",
        "\n",
        "        model.eval()\n",
        "        val_loss_accum = 0\n",
        "        n = 0\n",
        "        with torch.no_grad():\n",
        "            for val_x, val_y in valid_loader:\n",
        "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
        "\n",
        "                valid_logits, val_loss = model(val_x, val_y)\n",
        "                n += 1\n",
        "                val_loss_accum += val_loss.item()\n",
        "                if n == 10:\n",
        "                    break\n",
        "        valid_loss = val_loss_accum / n\n",
        "\n",
        "        checkpoint_dir = log_dir + f\"{formatted_time}\"\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.mkdir(checkpoint_dir)\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"model_{step}_valid_loss_{valid_loss:.3f}.pt\")\n",
        "        checkpoint = {\n",
        "            'model': model.state_dict(),\n",
        "            'config': model.config,\n",
        "            'step': step,\n",
        "            'val_loss': valid_loss\n",
        "        }\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "        if master_process:\n",
        "            print(f\"step {step}  |  loss: {loss_accum.item():.2f}  |  val_loss: {valid_loss:.2f}  |  lr: {lr:.5f}  |  norm: {norm:.2f}  |  dt: {dt:.2f}s  |  tok/sec: {tokens_per_sec:.2f}\")\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} completed.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAY6gkyBtdjx",
        "outputId": "9f45060a-58ea-404e-debf-a99d0c56a0d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sp.encode('ejrnver')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "yugC40aA_4e4",
        "outputId": "cea48028-c122-42fb-be43-446f6a7a06b8"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "Out of range: piece id is out of range.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-41227cb9b7ff>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mgenerated_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mDecode\u001b[0;34m(self, input, out_type, num_threads)\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DecodeIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DecodePieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36m_DecodeIds\u001b[0;34m(self, ids)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_DecodeIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor__DecodeIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_DecodeIdsAsBytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Out of range: piece id is out of range."
          ]
        }
      ],
      "source": [
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Replace with actual starting token if needed\n",
        "\n",
        "# Generate tokens one by one\n",
        "max_tokens = 100  # Adjust as needed\n",
        "generated_tokens = context.tolist()[0]  # Start with the initial token\n",
        "\n",
        "for _ in range(max_tokens):\n",
        "    input_tensor = torch.tensor([generated_tokens], dtype=torch.long, device=device)\n",
        "    output = model.generate(input_tensor, max_new_tokens=1)  # Generate one token at a time\n",
        "\n",
        "    new_token = output[0, -1].item()  # Extract the last generated token\n",
        "    generated_tokens.append(new_token)  # Append to the sequence\n",
        "\n",
        "    # Decode and print the current generated sequence\n",
        "    print(enc.decode(generated_tokens), end=\" \", flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf7bNE25sUmg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}